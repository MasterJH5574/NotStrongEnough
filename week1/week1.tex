\documentclass[UTF8, a4paper, linespread=1.5]{article}

\usepackage{tcolorbox, listings, algorithm, minted, algorithmic}
\usepackage{geometry, savesym, amsmath, enumerate, indentfirst, color, amsthm, bm, extarrows, ulem}
\usepackage{amssymb}
\usepackage{nameref, hyperref}
 \geometry{top=3cm, bottom=3cm, left=1.5cm, right=1.5cm}

\usepackage{enumitem}
\setenumerate[1]{itemsep=0pt,partopsep=0pt,parsep=\parskip,topsep=5pt}
\setitemize[1]{itemsep=0pt,partopsep=0pt,parsep=\parskip,topsep=5pt}

% \usepackage{adjustbox}

\renewcommand\contentsname{Contents}

\tcbuselibrary{skins, breakable, theorems}

% \setlength{\leftskip}{10pt}
\setlength{\parindent}{10pt}
% \setlength{\parskip}{2em}
\renewcommand{\baselinestretch}{1.3}

\newcounter{RomanNumber}
\newcommand{\mrm}[1]{(\setcounter{RomanNumber}{#1}\Roman{RomanNumber})}

\newtcbtheorem{thm}{}
  {enhanced, theorem name and number, code={\edef\@currentlabelname{#2}}, 
  frame code={
        % \path[thick, draw] (frame.north west) -| (frame.north east) -| (frame.south east) -| (frame.south west) -| (frame.north west);
        \path[thick, draw] (frame.north west)  +(.5\baselineskip,0) -| +(0,-.5\baselineskip);
        % \path[thick, draw] (frame.north east) +(-.5\baselineskip,0) -| +(0,-.5\baselineskip);
        % \path[thick, draw] (frame.south west) +(.5\baselineskip,0) -| +(0,.5\baselineskip);
        \path[thick, draw] (frame.south east) +(-.5\baselineskip,0) -| +(0,.5\baselineskip);
    },
    left=1mm, right=1mm, top=1mm, bottom=1mm,
    colback=black!5,
    colframe=red!75!black,
    colbacktitle=black!0,
    coltitle=black!100,
    fonttitle=\bfseries}{thm}


\usepackage{xparse}
\NewDocumentEnvironment{qte}{m}{\begin{tcolorbox}[breakable, leftrule=2mm, rightrule=-0.1mm, toprule=-0.1mm, bottomrule=-0.1mm, arc=0mm, colframe=black!30!white, colback=white, coltext=white!50!black]}{\\\rightline{#1}\end{tcolorbox}}

\title{CS217 -- Algorithm Design and Analysis \\ Homework 1}
\date{\today}
\author{Not Strong Enough}

\begin{document}
\maketitle

\begin{thm}{}{}
    Prove the more precise bound of the school method.
\end{thm}
\begin{proof}
	Assume $a$ has $n$ bits and $b$ has $k$ bits($k < n$), in the school method of division, 
	calculate in rounds. 
	
	In each round, extend $b$ by filling zeros into lower bits and make $b$ have same bit size as $a$. 
	If the extended $b$ is smaller than $a$, let $a$ minus $b$. 
	
	It is obvious that the first bit of $b$ is 1, so each minus will decrease the bit size of $a$. 
	Besides, since the minus is made only when the extended $b$ is smaller than the current value of $a$, 
	if the bit size of $a$ is smaller than $k$, the calculation will end. 
	So there are at most $n-k+1$ rounds. 
	
	Now consider the minus. Since the lower bits of extended $b$ are 0, only the first $k$ bits of extended $b$ 
	costs. So each minus has $k$ operations. 
	
    In all, there are at most $(n-k+1)*k$ operations, so the complexity is $O(k(n-k+1))=O(k(n-k))$ when $n\neq k$. 
    (but if $n=k$, $O(k(n-k))=O(0)$, but sometimes the method has some operations: e.g., $a=11$ and $b=10$, 
    there are 2 operations, which is not $O(0)$. )
\end{proof}

\newpage
\begin{thm}{}{}
    Prove the complexity of Euclid algorithm is $O(n^2)$
\end{thm}
\begin{proof}
	In each round, the Euclid algorithm calculates $a\%b$(assume $a\geq b$) which is less than $b$. 
	If the result is not 0, it uses the result with b to do the new round. So we can assume that, in each round, 
	the two calculated numbers are: 
	$$(x_0, x_1), (x_1, x_2)\dots (x_{m-1}, x_m)$$
	Where $x_0>x_1>\dots x_m, x_{m-1}\%x_m=0$. 
	Let the bit size of $x_i$ be $t_i$, then $t_0=n, t_1=k$, 
	using the result of Exercise 1 we can find that the total operation number is: 
    \begin{align}
    O(t_1(t_0-t_1))+O(t_2(t_1-t_2))+\dots + O(t_m(t_{m-1}-t_m)) &=O(\sum_{i=1}^m t_i t_{i-1}-\sum_{i=1}^m t_i^2)\\
    &<O(\sum_{i=0}^{m-1} t_i^2 -\sum_{i=1}^m t_i^2)\\
    &=O(t_0^2-t_m^2)=O(n^2)
    \end{align}
	So we can see the operation number of Euclid algorithm is $O(n^2)$
\end{proof}

\newpage
\begin{thm}{A Recursive Algorithm for the Binomial Coefficient}{}
   Using pseudocode, write a recursive algorithm computing
  ${n \choose k}$. Implement it in python! What is 
  the running time of your algorithm, in terms of $n$ and $k$? Would you say it is an efficient
  algorithm? Why or why not?
\end{thm}

\begin{algorithm}
    \caption{Binom\_BF: A Recursive Algorithm for the Binomial Coefficient.}
    \begin{algorithmic}
        \IF{$k=0$ or $k=n$}
        \RETURN $1$
        \ELSE
        \RETURN $Binom\_BF(n-1, k-1) + Binom\_BF(n-1, k)$
        \ENDIF
    \end{algorithmic}
\end{algorithm}

\begin{minted}{Python}
def binom_BF(n, k):
    if k==0 or k==n:
        return 1
    else:
        return binom_BF(n-1, k-1) + binom_BF(n-1, k)
\end{minted}

In order to count the number of operations in the recursive algorithm to calculate $\displaystyle \binom n k$. We shall take a look at how the values are calculated. While recursively calculating Fibonacci number $F_n$ can be represented by a tree, we can show this process by a grid graph. A value $\displaystyle \binom n k$, in grid $(n, k)$, is obtained by $(n-1, k-1)$ and $(n-1, k)$. This indicates that there are edges from $(n-1, k-1)$ and $(n-1, k)$ to $(n, k)$.

The recursive algorithm actually tries all possible paths to $(n, k)$, starting at $(i, j)$ where $j = 0$ or $j = i$. That is to say, the time that value is merged as $\displaystyle \binom i j = \binom {i-1}{j-1} + \binom {i-1} j$, is the number of paths going through $(i, j)$, which, by the definition of binomial coefficients, is $\displaystyle \binom {n-i}{k-j}$.

Considering the complexity of integer addition $O(\log \text{ bitsize})$, we can get the running time of the recursive algorithm: $$\sum_{i=0}^n\sum_{j=1}^{i-1} \binom {n-i}{k-j} \log  \binom i j < \sum_{i=0}^n\sum_{j=1}^{i-1} \binom {n-i}{k-j} \log  \binom n {\left\lfloor \frac{n}{2} \right\rfloor }.$$

Using the Stirling Formula, we have $\log n! = \Theta (n \log n)$,
$$O(\log \binom {2n}n) = O ((2n+\frac{1}{2})\log 2n-(n+\frac{1}{2})\log n-(n+\frac{1}{2})\log n) = O(n).$$

The upper bound is
$$O(\sum_{i=0}^n\sum_{j=1}^{i-1} \binom {n-i}{k-j} \log  \binom n {\left\lfloor \frac{n}{2} \right\rfloor }) = O(n \binom n k).$$

And the lower bound is
$$\Omega(\binom nk).$$

This algorithm is not efficient, because it cannot be done in polynomial time.
\newpage
\begin{thm}{A Dynamic Programming Algorithm for the Binomial Coefficient}{}
    Using pseudocode, 
    write a dynamic programming algorithm computing  $\binom{n}{k}$.  Implement it in python!  What is it running 
    time in terms of $n$ and $k$? Would you say your algorithm is efficient?  Why or why not?
    
\end{thm}
\begin{algorithm}
    \caption{Caluculate Binomial Coefficient Using DP}
    \begin{algorithmic}
        \STATE Create a 2-dimension array $arr$
        \FOR{$i=0$ to $n$}
        \STATE $arr[i][0]=1$
        \ENDFOR
        \FOR{$i=0$ to $k$}
        \STATE $arr[i][i]=1$
        \ENDFOR
        \FOR{$i=1$ to $n$}
        \FOR{$j=1$ to min($i-1$,$k$)}
        \STATE $arr[i][j]=arr[i-1][j]+arr[i-1][j-1]$
        \ENDFOR
        \ENDFOR
        \RETURN $arr[n][k]$
    \end{algorithmic}
\end{algorithm}
\begin{minted}{Python}
def calc_dp(n,k):
    arr = [[0 for i in range(k+1)] for j in range(n+1)]
    for i in range(n+1):
        arr[i][0]=1
    for i in range(k+1):
        arr[i][i]=1
    for i in range(1,n+1):
        for j in range(1,min(i-1,k)+1):
            arr[i][j]=arr[i-1][j]+arr[i-1][j-1]
return arr[n][k]
\end{minted}
Complexity analysis:

 When we traverse the array and visit \texttt{arr[i][j]}, we actually perform the add operation $\binom{i}{j}=\binom{i-1}{j}+\binom{i-1}{j-1}$. So the operation costs O($\log\binom{i}{j}$) time. Using the Stirling Formula, we can estimate $\log\binom{i}{j}\approx(i+\frac{1}{2})\log i-(i-j+\frac{1}{2})\log (i-j)-(j+\frac{1}{2})\log j =$ O($i$). The number of nodes we visit is O($k\cdot (2n-k)/2$)=O($kn$). Since we will only visit \texttt{arr[i][j]} once, we can estimate the total complexity as below: the upper bound is $O(kn\cdot n)$=$O(kn^2)$, the lower bound is $\Omega(kn)$. The algorithm is efficient, because it can be completed in polynomial time and there are no redundant operations.

Also, we can get the upper bound of the running time in another way:
\begin{align*}
    O(\sum_{i=0}^n \sum_{j=0}^{\min(i, k)} \log \binom ij) &= O((\sum_{i=0}^k\sum_{j=0}^i + \sum_{i=k+1}^n\sum_{j=0}^k) \log \binom ij)
.\end{align*}

We have
\begin{align*}
    \sum_{i=0}^k\sum_{j=0}^i \binom ij &= \sum_{i=1}^k\sum_{j=1}^i (i\log i - j \log j - (i-j)\log(i-j)) + o(k^3) \\
                                     &= \sum_{i=1}^k (i^2\log i + (\frac{i^2}{4}-\frac{i^2}{2}\log i) + (\frac{i^2}{4}-\frac{i^2}{2}\log i) ) + o(k^3)\\
                                     &= O(k^3)
.\end{align*}

And
\begin{align*}
    \sum_{i=k}^n\sum_{j=0}^k (i\log i - j \log j - (i-j)\log(i-j)) &= \sum_{i=k}^n (ki\log i - (\frac{k^2}{2}\log k - \frac{k^2}{4}) \\
                                                                 &\quad \quad \quad - (\frac{i^2}{2}\log i - \frac{i^2}{4}) + \frac{(i-k)^2}{2}\log(i-k)-\frac{(i-k)^2}{4}) + o(nk^2\log n) \\
                                                                 &= O(nk^2\log n)
.\end{align*}

Thus the running time is $O(nk^2\log n)$.
 
 \newpage
 \begin{thm}{Binomial Coefficient modulo 2}{}
     Suppose we are only interested in whether $\binom{n}{k}$ is even or odd, i.e., we want to compute $\binom{n}{k}$ mod 2. You could do this by computing  $\binom{n}{k}$ using dynamic programming and then taking the result modulo 2. What is the running time? Would you say this algorithm is efficient? Why or why not?
    
 \end{thm}

The running time is the same as Problem 4. It's not efficient, because it cannot be done in polynomial time corresponding to the output size. However, we can do modulo operation after every addition, so that the complexity of addition can be reduced to $O(1)$, so that the total complexity can be $O(n\cdot k)$. 

Furthermore, we can introduce a new algorithm to compute this problem(See Algorithm 3).
\begin{algorithm}
	\caption{Caluculate Binomial Coefficient Modulo 2}
	\begin{algorithmic}
		\STATE $factor$ = an empty array
		\FOR{$i=0$ to $n$}
		\STATE $factor[i]= calculateFactor2(i)$
		\ENDFOR
		\FOR{$i=1$ to $n$}
		\STATE $factor[i]= factor[i]+factor[i-1]$
		\ENDFOR
		\STATE $factorLeft=factor[n]-factor[n-k]-factor[k]$
		\IF{ $factorLeft>0$}
		\RETURN $0$
		\ELSE 
		\RETURN $1$
		\ENDIF
	\end{algorithmic}
\end{algorithm}
\begin{algorithm}
	\caption{calculateFactor2}
	\begin{algorithmic}
		\REQUIRE 
		the number i to be calculated 
		\IF {$i==0$}
		\RETURN 0
		\ENDIF
		\STATE $cnt=0$
		\WHILE {the lowest bit in $i$ is 0}
		\STATE $i=i>>1$
		\STATE $cnt=cnt+1$
		\ENDWHILE
		\RETURN $cnt$
	\end{algorithmic}
\end{algorithm}

The main idea is to calculate the number of factor 2 in $\binom{n}{k}$, equivalently, calculating the number of factor 2 in $ n!$, $(n-k)!$ and $k!$, based on the fact that $\binom{n}{k} = \frac{n!}{(n-k)!\cdot k!}$. 

The calculateFactor2(i) function is $O(\log i)$. The addition of \texttt{factor[i]} and \texttt{factor[i-1]} is also  $O(\log i)$, because \texttt{factor[i]} represents the number of factor 2 in $i!$, which is $O(i)$. So the total complexity should be:
\begin{align}
	O(\sum_{i=1}^{n}\log i)=O(\log(n!))=O(n\log n)
\end{align}

\newpage
\begin{thm}{}{}
    Remember the ``period'' algorithm for computing $F'_n := (F_n \mod k)$ discussed in class: (1) find some $i,j$ between $0$ and $k^2$ for which $F'_{i} =  F'_{j}$ and $F'_{i+1} = F'_{j+1}$. Then for $d := j-i$ the sequence $F'_{n}$ will repeat every $d$ steps, as there will be a cycle. This cycle can either be a ``true cycle'' or a ``lasso''. Show that a lasso cannot happen. That is, show that the smallest $i$ for which this happens is $0$.
\end{thm}

\begin{proof}
    We prove it by contradiction.
    
    Let $i_0 > 0$ be the smallest $i$ for which this happen, i.e, for some $j > 0$ we have: for $\forall p \geqslant i_0 > 0, F'_{p + j} = F'_p$. Since for $p \geqslant 1$, the fibonacci numbers are defined as $F_{p + 1} = F_{p} + F_{p - 1}$, we have $F'_{p + 1} = (F'_p + F'_{p - 1}) \mod k$. It follows that $F'_{p - 1} = (F'_{p + 1} - F'_p + k) \mod k$.
    
    Because $F'_{i_0} = F'_{i_0 + j}, F'_{i_0 + 1} = F'_{i_0 + 1 + j}$, we have
    \begin{align*}
        F'_{i_0 - 1} &= (F'_{i_0 + 1} - F'_{i_0} + k) \mod k \\
        &= (F'_{i_0 + 1 + j} - F'_{i_0 + j} + k) \mod k \\
        &= F'_{i_0 - 1 + j}
    \end{align*}
    , which is contradict with out hypothesis.
    
    Thus, $i_0$ isn't the smallest $i$, and we can conclude that the smallest $i$ is $0$.
\end{proof}

\end{document}

